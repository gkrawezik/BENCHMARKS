Different benchmarks that can be used on the machines at FI. Note that it is easy to overwhelm the batch schedulers, so try and make sure to not run something that will launch hundreds of jobs. Using [disBatch](https://github.com/flatironinstitute/disBatch) is a good idea if you can (single node)

# JUBE: 
These are different benchmarks to be used with the JUBE software
[Download JUBE here](https://www.fz-juelich.de/ias/jsc/EN/Expertise/Support/Software/JUBE/_node.html)

The idea is to provide an input file describing what you want to test (eg: different inputs, number of nodes, compilers, libraries...) and JUBE will run a matrix of all the different possible combinations, then present the results in a structured way.

A typical run will look like this:
```
jube run definition.xml        # Launch the benchmark
jube definition npb_omp --id 0 # Check periodically the progress of the benchmark
jube result definition --id 0  # Get the results
```

These folders do not contain any packages: you might need to download them if they are not present in the modules system.
Input files, XML files for the benchmarks, and Slurm templates are provided.

The main goal is to show different ways of using JUBE, to check performance and scaling, compiler settings, environment settings, etc

## GADGET4: Gadget4 _Weak scaling and compilers_
This test is of interest for anyone who wants to compare the performance of the code generated by different compilers. The benchmark will start by compiling the code, then run on different problem sizes, increasing with the number of processors

## GROMACS: Gromacs _Strong scaling, GPUs, and OpenMP/MPI mix_
This test can be used for scalability testing, testing different ranks/threads configurations, on both GPU and CPU based clusters, for different inputs

## HPCG: High Performance Conjugate Gradient _Weak scaling, GPUs, singularity/docker container_
This test contains both the (non optimized) reference code for HPCG, but also the NVIDIA provided one for GPUs. This shows how to use singularity.

## HPL: High Performance Linpack _Weak scaling, GPUs, singularity/docker container_
This test contains also the reference code, as well as the NVIDIA optimized version which has to be run through a singularity

## NPB: NAS Parallel Benchmarks _Strong scaling, disBatch, MPI, OpenMP_
Different versions of the NPB:
* single-node disBatch: when running with up to *number of cores per node*, we can run our suite using disBatch, this is such an example. Submit with mpi\_singlenode\_disbatched.slurm instead of jube directly
* Generic MPI and OpenMP:
  * Single-node
  * Multi-node (MPI only)

